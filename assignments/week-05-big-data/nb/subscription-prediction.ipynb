{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmil5P4G6052"
   },
   "source": [
    "<p align = \"center\" draggable=”false” ><img src=\"https://user-images.githubusercontent.com/37101144/161836199-fdb0219d-0361-4988-bf26-48b0fad160a3.png\" \n",
    "     width=\"200px\"\n",
    "     height=\"auto\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f25fead1-c5a9-4222-a41c-5d1634dd3446",
     "showTitle": false,
     "title": ""
    },
    "id": "oQU8pT9m6055"
   },
   "source": [
    "# <h1 align=\"center\" id=\"heading\">Subscription Prediction with Delta Lake, PySpark, and MLlib</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxs4CnZ8MYR9"
   },
   "source": [
    "# Spark Environment\n",
    "\n",
    "Make sure that you open this notebook in your Spark environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "i5lvRImyZQKy"
   },
   "outputs": [],
   "source": [
    "# !pip install -U -q pyspark delta-spark # If you use Colab uncomment this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B30ILkiC6056"
   },
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dvbp4ifI6057"
   },
   "source": [
    "At the end of this session, you will be able to \n",
    "\n",
    "- Load, save, partition data with Delta Lake tables\n",
    "- Explore data with Spark DataFrames \n",
    "- Build a pipeline in MLlib for machine learning workflow\n",
    "- Fit a logistic regression model, make predictions, and evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2571c03a-cae9-4b9b-9861-f040eb696b2a",
     "showTitle": false,
     "title": ""
    },
    "id": "tTvh8EkV6057"
   },
   "source": [
    "## Part 1: Data Loader\n",
    "\n",
    "We are using a dataset from the UCI Machine Learning Repository.\n",
    "\n",
    "1. Use `wget` to download the dataset. Then use `ls` to verify that the `bank.zip` file is downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z0Su1lUX6058",
    "outputId": "dcf3d53c-06ad-44d6-db86-5c47bd11420e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-01-19 18:12:58--  https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 579043 (565K) [application/x-httpd-php]\n",
      "Saving to: ‘bank.zip’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  8%  271K 2s\n",
      "    50K .......... .......... .......... .......... .......... 17%  531K 1s\n",
      "   100K .......... .......... .......... .......... .......... 26% 46.6M 1s\n",
      "   150K .......... .......... .......... .......... .......... 35%  485K 1s\n",
      "   200K .......... .......... .......... .......... .......... 44% 2.35M 1s\n",
      "   250K .......... .......... .......... .......... .......... 53% 2.36M 0s\n",
      "   300K .......... .......... .......... .......... .......... 61% 93.7M 0s\n",
      "   350K .......... .......... .......... .......... .......... 70% 1.27M 0s\n",
      "   400K .......... .......... .......... .......... .......... 79% 8.60M 0s\n",
      "   450K .......... .......... .......... .......... .......... 88% 62.4M 0s\n",
      "   500K .......... .......... .......... .......... .......... 97% 60.3M 0s\n",
      "   550K .......... .....                                      100% 64.3M=0.5s\n",
      "\n",
      "2023-01-19 18:13:00 (1.17 MB/s) - ‘bank.zip’ saved [579043/579043]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "wget https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "14241a78-c5ad-4962-9322-e0e6bdf3737f",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0gAxRiyy605-",
    "outputId": "9023a057-930e-458c-dd8e-2493d8bc8120"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bank-full.csv                  bank.zip\n",
      "bank-names.txt                 imports.ipynb\n",
      "bank.csv                       subscription-prediction.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGBxoptx605-"
   },
   "source": [
    "2. Unzip the file and use `ls` to see the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "000511fa-ff85-406c-9186-34c56a0369a6",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hySwTxHs605_",
    "outputId": "f9ac5ddd-807a-49e0-ca45-bc00e3bb8ac6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  bank.zip\n",
      "  inflating: bank-full.csv           \n",
      "  inflating: bank-names.txt          \n",
      "  inflating: bank.csv                \n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "unzip bank.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2349ba91-fc61-456f-96bf-5f5aea6b32d1",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K6UDQW1y605_",
    "outputId": "4a312342-54ef-4722-98b0-38005bbcd06f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 11144\n",
      "-rw-r--r--  1 wildfell  staff   4.4M Feb 14  2012 bank-full.csv\n",
      "-rw-r--r--  1 wildfell  staff   3.8K Feb 14  2012 bank-names.txt\n",
      "-rw-r--r--  1 wildfell  staff   451K Feb 14  2012 bank.csv\n",
      "-rw-r--r--  1 wildfell  staff   565K Feb 14  2012 bank.zip\n",
      "-rw-r--r--  1 wildfell  staff   1.9K Jan 19 18:11 imports.ipynb\n",
      "-rw-r--r--  1 wildfell  staff    39K Jan 19 17:29 subscription-prediction.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Inspect the dataset and note if there is anything to be aware of in the dataset structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bsKTEA_kJm1Z",
    "outputId": "cc90db82-213d-4362-c8c2-38ab9373abbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    4522 bank.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "wc -l bank.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PUOdYkMKRrFf",
    "outputId": "61dd1ff6-8d43-4e2a-f04f-94b6bc3da874"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"age\";\"job\";\"marital\";\"education\";\"default\";\"balance\";\"housing\";\"loan\";\"contact\";\"day\";\"month\";\"duration\";\"campaign\";\"pdays\";\"previous\";\"poutcome\";\"y\"\n",
      "30;\"unemployed\";\"married\";\"primary\";\"no\";1787;\"no\";\"no\";\"cellular\";19;\"oct\";79;1;-1;0;\"unknown\";\"no\"\n",
      "33;\"services\";\"married\";\"secondary\";\"no\";4789;\"yes\";\"yes\";\"cellular\";11;\"may\";220;1;339;4;\"failure\";\"no\"\n",
      "35;\"management\";\"single\";\"tertiary\";\"no\";1350;\"yes\";\"no\";\"cellular\";16;\"apr\";185;1;330;1;\"failure\";\"no\"\n",
      "30;\"management\";\"married\";\"tertiary\";\"no\";1476;\"yes\";\"yes\";\"unknown\";3;\"jun\";199;4;-1;0;\"unknown\";\"no\"\n",
      "59;\"blue-collar\";\"married\";\"secondary\";\"no\";0;\"yes\";\"no\";\"unknown\";5;\"may\";226;1;-1;0;\"unknown\";\"no\"\n",
      "35;\"management\";\"single\";\"tertiary\";\"no\";747;\"no\";\"no\";\"cellular\";23;\"feb\";141;2;176;3;\"failure\";\"no\"\n",
      "36;\"self-employed\";\"married\";\"tertiary\";\"no\";307;\"yes\";\"no\";\"cellular\";14;\"may\";341;1;330;2;\"other\";\"no\"\n",
      "39;\"technician\";\"married\";\"secondary\";\"no\";147;\"yes\";\"no\";\"cellular\";6;\"may\";151;2;-1;0;\"unknown\";\"no\"\n",
      "41;\"entrepreneur\";\"married\";\"tertiary\";\"no\";221;\"yes\";\"no\";\"unknown\";14;\"may\";57;2;-1;0;\"unknown\";\"no\"\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "head bank.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some libraries\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>married</td>\n",
       "      <td>primary</td>\n",
       "      <td>no</td>\n",
       "      <td>1787</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>19</td>\n",
       "      <td>oct</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>4789</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>cellular</td>\n",
       "      <td>11</td>\n",
       "      <td>may</td>\n",
       "      <td>220</td>\n",
       "      <td>1</td>\n",
       "      <td>339</td>\n",
       "      <td>4</td>\n",
       "      <td>failure</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "      <td>management</td>\n",
       "      <td>single</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>1350</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>16</td>\n",
       "      <td>apr</td>\n",
       "      <td>185</td>\n",
       "      <td>1</td>\n",
       "      <td>330</td>\n",
       "      <td>1</td>\n",
       "      <td>failure</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>1476</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>3</td>\n",
       "      <td>jun</td>\n",
       "      <td>199</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>226</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4516</th>\n",
       "      <td>33</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>-333</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>30</td>\n",
       "      <td>jul</td>\n",
       "      <td>329</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4517</th>\n",
       "      <td>57</td>\n",
       "      <td>self-employed</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>yes</td>\n",
       "      <td>-3313</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>9</td>\n",
       "      <td>may</td>\n",
       "      <td>153</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4518</th>\n",
       "      <td>57</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>295</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>19</td>\n",
       "      <td>aug</td>\n",
       "      <td>151</td>\n",
       "      <td>11</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4519</th>\n",
       "      <td>28</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>1137</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>6</td>\n",
       "      <td>feb</td>\n",
       "      <td>129</td>\n",
       "      <td>4</td>\n",
       "      <td>211</td>\n",
       "      <td>3</td>\n",
       "      <td>other</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4520</th>\n",
       "      <td>44</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>single</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>1136</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>cellular</td>\n",
       "      <td>3</td>\n",
       "      <td>apr</td>\n",
       "      <td>345</td>\n",
       "      <td>2</td>\n",
       "      <td>249</td>\n",
       "      <td>7</td>\n",
       "      <td>other</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4521 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age            job  marital  education default  balance housing loan  \\\n",
       "0      30     unemployed  married    primary      no     1787      no   no   \n",
       "1      33       services  married  secondary      no     4789     yes  yes   \n",
       "2      35     management   single   tertiary      no     1350     yes   no   \n",
       "3      30     management  married   tertiary      no     1476     yes  yes   \n",
       "4      59    blue-collar  married  secondary      no        0     yes   no   \n",
       "...   ...            ...      ...        ...     ...      ...     ...  ...   \n",
       "4516   33       services  married  secondary      no     -333     yes   no   \n",
       "4517   57  self-employed  married   tertiary     yes    -3313     yes  yes   \n",
       "4518   57     technician  married  secondary      no      295      no   no   \n",
       "4519   28    blue-collar  married  secondary      no     1137      no   no   \n",
       "4520   44   entrepreneur   single   tertiary      no     1136     yes  yes   \n",
       "\n",
       "       contact  day month  duration  campaign  pdays  previous poutcome   y  \n",
       "0     cellular   19   oct        79         1     -1         0  unknown  no  \n",
       "1     cellular   11   may       220         1    339         4  failure  no  \n",
       "2     cellular   16   apr       185         1    330         1  failure  no  \n",
       "3      unknown    3   jun       199         4     -1         0  unknown  no  \n",
       "4      unknown    5   may       226         1     -1         0  unknown  no  \n",
       "...        ...  ...   ...       ...       ...    ...       ...      ...  ..  \n",
       "4516  cellular   30   jul       329         5     -1         0  unknown  no  \n",
       "4517   unknown    9   may       153         1     -1         0  unknown  no  \n",
       "4518  cellular   19   aug       151        11     -1         0  unknown  no  \n",
       "4519  cellular    6   feb       129         4    211         3    other  no  \n",
       "4520  cellular    3   apr       345         2    249         7    other  no  \n",
       "\n",
       "[4521 rows x 17 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pandas Profiling Report\n",
      "\n",
      "\n",
      "Describe data frame\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>balance</th>\n",
       "      <th>day</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>41.170095</td>\n",
       "      <td>1422.657819</td>\n",
       "      <td>15.915284</td>\n",
       "      <td>263.961292</td>\n",
       "      <td>2.793630</td>\n",
       "      <td>39.766645</td>\n",
       "      <td>0.542579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.576211</td>\n",
       "      <td>3009.638142</td>\n",
       "      <td>8.247667</td>\n",
       "      <td>259.856633</td>\n",
       "      <td>3.109807</td>\n",
       "      <td>100.121124</td>\n",
       "      <td>1.693562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>-3313.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>444.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>49.000000</td>\n",
       "      <td>1480.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>329.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>87.000000</td>\n",
       "      <td>71188.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>3025.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>871.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               age       balance          day     duration     campaign  \\\n",
       "count  4521.000000   4521.000000  4521.000000  4521.000000  4521.000000   \n",
       "mean     41.170095   1422.657819    15.915284   263.961292     2.793630   \n",
       "std      10.576211   3009.638142     8.247667   259.856633     3.109807   \n",
       "min      19.000000  -3313.000000     1.000000     4.000000     1.000000   \n",
       "25%      33.000000     69.000000     9.000000   104.000000     1.000000   \n",
       "50%      39.000000    444.000000    16.000000   185.000000     2.000000   \n",
       "75%      49.000000   1480.000000    21.000000   329.000000     3.000000   \n",
       "max      87.000000  71188.000000    31.000000  3025.000000    50.000000   \n",
       "\n",
       "             pdays     previous  \n",
       "count  4521.000000  4521.000000  \n",
       "mean     39.766645     0.542579  \n",
       "std     100.121124     1.693562  \n",
       "min      -1.000000     0.000000  \n",
       "25%      -1.000000     0.000000  \n",
       "50%      -1.000000     0.000000  \n",
       "75%      -1.000000     0.000000  \n",
       "max     871.000000    25.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(pd.read_csv('bank.csv' , sep= ';'))\n",
    "\n",
    "print('\\nPandas Profiling Report\\n')\n",
    "# profile = ProfileReport( pd.read_csv('bank.csv' , sep= ';') , title=\"Pandas Profiling Report\")\n",
    "# profile.to_widgets()\n",
    "\n",
    "print('\\nDescribe data frame\\n')\n",
    "pd.read_csv('bank.csv' , sep= ';').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CLp_YaaNRvB"
   },
   "source": [
    "4. Create a Delta table for `bank.csv` (Why Delta Lake? Here's a read [here](https://medium.com/@databeans-blogs/delta-lake-the-data-engineers-missing-piece-part-1-ebab66a3f8c0?source) from a data engineer's perspective)\n",
    "\n",
    "    We first set up a Python project `ml-bank`, configure the SparkSession with the `configure_spark_with_delta_pip()` utility function in Delta Lake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2VvPSH-2DVMy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/19 19:35:08 WARN Utils: Your hostname, Wildfells-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.86.46 instead (on interface en0)\n",
      "23/01/19 19:35:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/Users/wildfell/miniconda3/envs/sentiment_analysis/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/wildfell/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/wildfell/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-cac3fe5b-adb7-40df-b293-ca69265e5785;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.2.0 in central\n",
      "\tfound io.delta#delta-storage;2.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.2.0/delta-core_2.12-2.2.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-core_2.12;2.2.0!delta-core_2.12.jar (507ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/2.2.0/delta-storage-2.2.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;2.2.0!delta-storage.jar (95ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.8/antlr4-runtime-4.8.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.8!antlr4-runtime.jar (108ms)\n",
      ":: resolution report :: resolve 2408ms :: artifacts dl 726ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-cac3fe5b-adb7-40df-b293-ca69265e5785\n",
      "\tconfs: [default]\n",
      "\t3 artifacts copied, 0 already retrieved (3728kB/85ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/19 19:35:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"ml-bank\") \\\n",
    "  .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "# to run this next line I needed to download Java. The brew install openjdk@11 command didnt work.\n",
    "# fixing keg-only / adding paths / points didn't resolve the error.\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaHADc7XPgTA"
   },
   "source": [
    "5. We define output formats and paths, you need to complete code to load the data from its source (since the delimiter of the file is semicolon, use [`spark.read.csv`](https://spark.apache.org/docs/latest/sql-data-sources-csv.html) that provides more flexibility) and write the data to its target (hint: [Create a table](https://docs.databricks.com/delta/delta-batch.html)).\n",
    "\n",
    "\n",
    "__NOTE__ Databricks requires all the paths to be absolute, not relative. To check the absolute path, run `pwd` bash command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Cell\n",
    "# # Define the input and output formats and paths and the table name.\n",
    "# write_format = 'delta'\n",
    "# load_path = 'file:/databricks/driver/bank.csv'\n",
    "# save_path = 'file:/databricks/driver/tmp/delta/bank-4k'\n",
    "# table_name = 'default.bank4k'\n",
    "\n",
    "# # Load the data from its source into a dataframe.\n",
    "# # [YOUR CODE HERE]\n",
    "\n",
    "# # Create table with path using DataFrame's schema and write data to it\n",
    "# # Note if you are overwriting to specificy overwrite option\n",
    "# # [YOUR CODE HERE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/wildfell/Desktop/fourthbrain_mle/fourthbrain-projects/assignments/week-05-big-data/nb\n"
     ]
    }
   ],
   "source": [
    "%%sh \n",
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot write to already existent path file:/Users/wildfell/Desktop/fourthbrain_mle/fourthbrain-projects/assignments/week-05-big-data/nb/tmp/delta/bank-4k without setting OVERWRITE = 'true'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m;\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39mheader\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39mheader\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mcsv(load_path, header\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m \u001b[39m# Create table with path using DataFrame's schema and write data to it\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m# Note if you are overwriting to specificy overwrite option\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m df\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mformat(write_format)\u001b[39m.\u001b[39;49msave(save_path)\n\u001b[1;32m     14\u001b[0m df\u001b[39m.\u001b[39mwrite\u001b[39m.\u001b[39mformat(write_format)\u001b[39m.\u001b[39mmode(\u001b[39m\"\u001b[39m\u001b[39moverwrite\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39msaveAsTable(table_name)\n",
      "File \u001b[0;32m~/miniconda3/envs/sentiment_analysis/lib/python3.8/site-packages/pyspark/sql/readwriter.py:968\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39msave()\n\u001b[1;32m    967\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave(path)\n",
      "File \u001b[0;32m~/miniconda3/envs/sentiment_analysis/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/miniconda3/envs/sentiment_analysis/lib/python3.8/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Cannot write to already existent path file:/Users/wildfell/Desktop/fourthbrain_mle/fourthbrain-projects/assignments/week-05-big-data/nb/tmp/delta/bank-4k without setting OVERWRITE = 'true'."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/20 06:30:37 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 153878 ms exceeds timeout 120000 ms\n",
      "23/01/20 06:30:37 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "# Define the input and output formats and paths and the table name.\n",
    "write_format = 'delta'\n",
    "load_path = '/Users/wildfell/Desktop/fourthbrain_mle/fourthbrain-projects/assignments/week-05-big-data/nb/bank.csv'\n",
    "save_path = '/Users/wildfell/Desktop/fourthbrain_mle/fourthbrain-projects/assignments/week-05-big-data/nb/tmp/delta/bank-4k'\n",
    "table_name = 'default.bank4k'\n",
    "\n",
    "# Load the data from its source into a dataframe.\n",
    "df = spark.read.option(\"delimiter\", \";\").option(\"header\", \"true\").option(\"header\", \"true\").csv(load_path, header=True)\n",
    "\n",
    "\n",
    "# Create table with path using DataFrame's schema and write data to it\n",
    "# Note if you are overwriting to specificy overwrite option\n",
    "df.write.format(write_format).save(save_path)\n",
    "df.write.format(write_format).mode(\"overwrite\").saveAsTable(table_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: string, job: string, marital: string, education: string, default: string, balance: string, housing: string, loan: string, contact: string, day: string, month: string, duration: string, campaign: string, pdays: string, previous: string, poutcome: string, y: string]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSPA8U_WUr8s"
   },
   "source": [
    "6. Verify what we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1jqA0JktUqg8",
    "outputId": "2dfea55f-a85f-4348-c55e-b1456e5b3cda"
   },
   "outputs": [],
   "source": [
    "# ls -lh /databricks/driver/tmp/delta/bank-4k/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 11184\n",
      "-rw-r--r--  1 wildfell  staff   4.4M Feb 14  2012 bank-full.csv\n",
      "-rw-r--r--  1 wildfell  staff   3.8K Feb 14  2012 bank-names.txt\n",
      "-rw-r--r--  1 wildfell  staff   451K Feb 14  2012 bank.csv\n",
      "-rw-r--r--  1 wildfell  staff   565K Feb 14  2012 bank.zip\n",
      "-rw-r--r--  1 wildfell  staff   1.9K Jan 19 18:11 imports.ipynb\n",
      "drwxr-xr-x  3 wildfell  staff    96B Jan 19 22:04 \u001b[1m\u001b[36mspark-warehouse\u001b[m\u001b[m/\n",
      "-rw-r--r--  1 wildfell  staff    59K Jan 19 19:38 subscription-prediction.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls -lh /Users/wildfell/Desktop/fourthbrain_mle/fourthbrain-projects/assignments/week-05-big-data/nb/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tz2noJLho8ME"
   },
   "source": [
    "7. Partition data by `job` status. \n",
    "\n",
    "  To speed up queries that have predicates involving the partition columns, we should partition data. Often time, we partition by anonymized user id; here we demonstrate the idea with `job`.\n",
    "\n",
    "__NOTE__ We saw previously that the data can be loaded in Delta lake as one table. Now to partition the data, we need to remove the existing directory, or load it into a different directoty. The goal is to showcase delta lake capabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJCHS1OxnoUu"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree('/databricks/driver/tmp/delta/bank-4k') # To replace data, we need to remove the existing directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_by = 'job'\n",
    "\n",
    "# Write the data to its target.\n",
    "# [YOUR CODE HERE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3N57HCGCoA6Y",
    "outputId": "6e0e381f-4c10-4f1c-87f3-6edc442a6b08"
   },
   "outputs": [],
   "source": [
    "ls -lh /databricks/driver/tmp/delta/bank-4k/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFRWT4qobDCw"
   },
   "source": [
    "  We only touch the surface of Delta Lake, for more information, check [Delta Lake guide](https://docs.databricks.com/delta/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9e212db6-6e87-468b-abd7-15a720ff8b96",
     "showTitle": false,
     "title": ""
    },
    "id": "d6e3Lpd8606A"
   },
   "source": [
    "## Part 2: Exploring The Data\n",
    "\n",
    "We will use the direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict whether the client will subscribe (Yes/No) to a term deposit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySKvPhY8ZOQx"
   },
   "source": [
    "1. Load data from its source by specifying the data format and path; then check out the schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "24bcf95a-d1ca-497c-9243-b0ab5556c6c4",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GVLwrGAN606A",
    "outputId": "97781c0e-cea0-4476-a2b7-12e294da3113"
   },
   "outputs": [],
   "source": [
    "read_format = 'delta'\n",
    "load_path = '/tmp/delta/bank-4k/'\n",
    "\n",
    "df = spark.read.format(read_format).load(load_path) \n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "08b5bb73-fc91-4240-a31c-036605467e8f",
     "showTitle": false,
     "title": ""
    },
    "id": "Aiqk3hQO606B"
   },
   "source": [
    "Here are the columns you should see:\n",
    "\n",
    "* Input variables: age, job, marital, education, default, balance, housing, loan, contact, day, month, duration, campaign, pdays, previous, poutcome\n",
    "\n",
    "* Output variable: y (deposit)\n",
    "\n",
    "2. Have a peek of the first five observations. Use the `.show()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CWPoxTh7606B",
    "outputId": "3287ef72-969f-463f-b922-46ed3d95dd46"
   },
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOITLLCvG4Sb"
   },
   "source": [
    "To get a prettier result, it can be nice to use Pandas to display our DataFrame. Use the Spark `.take()` method to get the first 5 rows and then convert to a pandas DataFrame. Don't forget to pass along the column names. You should see the same result as above, but in a more aesthetically appealing format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "VRenwPrOGz0D",
    "outputId": "3502ad2b-5d95-4f5c-b80c-e2e1fa68417b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# [YOUR CODE HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uKOs_Ka606B"
   },
   "source": [
    "3. We can also perform transformations on our DataFrame using the Pandas commands that we know and love using the [Pandas on Spark API](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html) **(new on Spark versions >= 3.2)**.  Pandas on Spark API was born out of the Databricks project, Koalas 🐨, allows us to use the Pandas commands and aesthetically pleasing output that we know and love distributed on the speed and scale of Spark!  For a nice quickstart on Pandas on Spark, check out this [article](https://towardsdatascience.com/run-pandas-as-fast-as-spark-f5eefe780c45)!  Let's convert our Spark DataFrame to a Pandas on Spark DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9abd6308-77f0-420b-a80a-03ef2928c963",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "p7G6371u606C",
    "outputId": "9a07865b-02c9-4c7f-a89e-c15b0265e6f4"
   },
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "psdf = df.pandas_api()\n",
    "psdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMK2KbvG606C"
   },
   "source": [
    "4. How many datapoints are there in the dataset? Use the `.count()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d2bccb04-2a21-4053-bac9-44db61d30a14",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CGCjBX-8606C",
    "outputId": "dbf49fd1-5cb4-458c-c7f8-188dd8d9ecb0"
   },
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6chyZha606D"
   },
   "source": [
    "5. Use the `.describe()` method to see summary statistics on the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 291
    },
    "id": "mpmp504K606D",
    "outputId": "97911142-6fb9-4ddc-f49c-363dfac124c4"
   },
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c7e85015-5739-4f67-bd73-af23d1decf4c",
     "showTitle": false,
     "title": ""
    },
    "id": "8hdB0-gZ606D"
   },
   "source": [
    "6. The above result includes the columns that are categorical, so those columns don't have useful summary statistics. Let's inspect just the numeric features.\n",
    "\n",
    "    `numeric_features` is defined below to contain the column names of the numeric features.  Notice we use the `zip` functions to iterate through two lists at the same time!\n",
    "    \n",
    "    Filter the DataFrame as you would in pandas to select only the numeric features from the DataFrame and then get the summary statistics on the resulting DataFrame as we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b843ce9c-92bb-4e45-b54a-4fea20d7e5d7",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 291
    },
    "id": "xP4uY29z606E",
    "outputId": "8fbfb144-ab13-41be-f43e-1137c256b5d8"
   },
   "outputs": [],
   "source": [
    "col_names = [name for name in psdf.dtypes.index]\n",
    "dtypes = [dtype for dtype in psdf.dtypes.tolist()]\n",
    "\n",
    "numeric_features = [name for name, dtype in zip(col_names, dtypes) if dtype == 'int32']\n",
    "# [YOUR CODE HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "72534767-044a-417d-957c-ab26679b103d",
     "showTitle": false,
     "title": ""
    },
    "id": "gOBPcOPe606E"
   },
   "source": [
    "7. Run the following code to look at correlation between the numeric features.  Let's convert our Pandas on Spark DataFrame to a Pandas DataFrame using the `to_pandas()` command. This will enable us to plot the data.  What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1cfda4ef-8aa3-4f69-8ca8-101d11f17e8e",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "id": "_3iYP4Ma606E",
    "outputId": "f719637a-43b2-4817-99b1-3293e5875b4a"
   },
   "outputs": [],
   "source": [
    "# Convert Pandas on Spark DataFrame to Spark DataFrame\n",
    "numeric_data = psdf[numeric_features].to_pandas()\n",
    "\n",
    "axs = pd.plotting.scatter_matrix(numeric_data, figsize=(8, 8));\n",
    "n = len(numeric_data.columns)\n",
    "\n",
    "for i in range(n):\n",
    "    v = axs[i, 0]\n",
    "    v.yaxis.label.set_rotation(0)\n",
    "    v.yaxis.label.set_ha('right')\n",
    "    v.set_yticks(())\n",
    "    h = axs[n - 1, i]\n",
    "    h.xaxis.label.set_rotation(90)\n",
    "    h.set_xticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "79d455fa-6ee2-4de6-b055-fece294607d9",
     "showTitle": false,
     "title": ""
    },
    "id": "oqJ0UpN7606E"
   },
   "source": [
    "There aren't any highly correlated variables, implying that we can keep them all for the model. However, day and month columns are not really useful, so will remove these two columns.\n",
    "\n",
    "8. Use the `.drop()` method to remove the `month` and `day` columns.\n",
    "    \n",
    "    Note that this method returns a new DataFrame, so save that result as `sdf`.\n",
    "\n",
    "    Use the `.dtypes` method to verify that `sdf` now has the correct columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "adef7a84-4270-4108-9849-ef699d0d7541",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BBiC1jtR606F",
    "outputId": "22b40379-bf5e-47a0-b338-5e936d58109b"
   },
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dc7c7c08-3090-4609-b0b3-a5ffa6be0981",
     "showTitle": false,
     "title": ""
    },
    "id": "NaVpHgTy606F"
   },
   "source": [
    "## Part 3: Preparing Data for Training a Model\n",
    "\n",
    "What follows is something analagous to a dataloader pipeline in Tensorflow--we're going to chain together some transformations that will convert our categorical variables into a one-hot format more amenable to training a machine learning model. \n",
    "The next code cell just sets this all up, but it doesn't run these transformations on our data yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "72e6e744-d740-4365-b02a-cec38e4b0779",
     "showTitle": false,
     "title": ""
    },
    "id": "gO45xEZ9606F"
   },
   "source": [
    "The process includes Category Indexing, One-Hot Encoding and VectorAssembler — a feature transformation that merges multiple columns into a vector column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c20c43bc-899c-46fd-8cd8-68f7faddd8b9",
     "showTitle": false,
     "title": ""
    },
    "id": "TIBGTMSU606G"
   },
   "source": [
    "The code is taken from [databricks’ official site](https://docs.databricks.com/applications/machine-learning/train-model/mllib/index.html#binary-classification-example) and it indexes each categorical column using the StringIndexer, then converts the indexed categories into one-hot encoded variables. \n",
    "The resulting output has the binary vectors appended to the end of each row. \n",
    "We use the StringIndexer again to encode our labels to label indices. \n",
    "Next, we use the VectorAssembler to combine all the feature columns into a single vector column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5Zs_fki606G"
   },
   "source": [
    "1. Complete the code by completing the assignment of `assembler`. Use `VectorAssembler` and pass in `assemblerInputs` as `inputCols` and name the `outputCol` `\"features\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2f4b6f3b-a25b-4c93-ae7b-d9fcf8d451df",
     "showTitle": false,
     "title": ""
    },
    "id": "beyBChtD606G"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder , StringIndexer, VectorAssembler\n",
    "\n",
    "categoricalColumns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']\n",
    "stages = []\n",
    "\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = 'y', outputCol = 'label')\n",
    "stages += [label_stringIdx]\n",
    "numericCols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = # [YOUR CODE HERE]\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0a430a65-f90a-4941-8476-7497416dd0b8",
     "showTitle": false,
     "title": ""
    },
    "id": "gsTuJQBk606G"
   },
   "source": [
    "## Part 4: Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e6191ae8-826c-405c-835b-85ccde4ed00d",
     "showTitle": false,
     "title": ""
    },
    "id": "mDNzrSSr606H"
   },
   "source": [
    "We use Pipeline to chain multiple transformations and estimators together to specify our machine learning workflow. \n",
    "A Pipeline’s stages are specified as an ordered array.  \n",
    "To run the pipeline on our Pandas on Spark DataFrame, we will convert it back to a Spark DataFrame using the **`to_spark()`** command\n",
    "\n",
    "1. Fit a pipeline on df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "sdf = sdf.to_spark()\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "pipelineModel = # [YOUR CODE HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FN6xXah5606H"
   },
   "source": [
    "2. Transform `pipelineModel` on `df` and assign this to variable `transformed_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QTsKIGt1606H",
    "outputId": "d42b5902-4ac8-4e97-80ab-1d29f4753250"
   },
   "outputs": [],
   "source": [
    "transformed_df = None # [YOUR CODE HERE]\n",
    "transformed_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fS3OQX0p606H"
   },
   "source": [
    "From the transformation, we'd like to take the `label` and `features` columns as well as the original columns from `sdf.`\n",
    "\n",
    "3. Use the `.select()` method to pull these columns from the `transformed_df` and reassign the resulting DataFrame to `sdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OWLr2mo0606I",
    "outputId": "4ebeaf4d-f3f3-4a67-9480-93455b92cff3"
   },
   "outputs": [],
   "source": [
    "selectedCols = ['label', 'features'] + sdf.columns\n",
    "sdf = None # [YOUR CODE HERE]\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCA4o74C606I"
   },
   "source": [
    "4. Let's view the first five rows of the `sdf` DataFrame using the methods we learned in Part 2:\n",
    "    * `.show()` method\n",
    "    * `.take()` method and convert result to a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b177c7ee-96a4-4340-952b-e02cb46faf25",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "id": "rUnHgPSY606I",
    "outputId": "ad6b8ce7-5306-416e-cd30-4b5dad1acbbe"
   },
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgtkTLyN606I"
   },
   "source": [
    "5. Randomly split the dataset in training and test sets, with 70% of the data in the training set and the remaining 30% in the test set.\n",
    "\n",
    "    Hint: Call the `.randomSplit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b4798af7-e77f-4ebb-891f-c814799f8618",
     "showTitle": false,
     "title": ""
    },
    "id": "pOV9t-nj606I"
   },
   "outputs": [],
   "source": [
    "train, test = None # [YOUR CODE HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ch88ygkE606J"
   },
   "source": [
    "6. What are the sizes of the training and test sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qYQ9e6Ha606J",
    "outputId": "2d73d0d2-decb-42de-e7ec-813bc2f1caf2"
   },
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "42af67c4-7257-40f0-8219-069494123190",
     "showTitle": false,
     "title": ""
    },
    "id": "IWNVbqgU606J"
   },
   "source": [
    "## Part 5: Logistic Regression Model\n",
    "\n",
    "Optional:\n",
    "- You can build a RandomForestClassifier with : from pyspark.ml.classification import RandomForestClassifier\n",
    "- You can build a Gradient-Boosted Tree Classifier with : from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "1. Fit a logistic regression with `featuresCol` as `\"features\"`, `labelCol` as `\"label\"` and a `maxIter` of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "45df93be-c36f-4b6c-8393-13e39311e218",
     "showTitle": false,
     "title": ""
    },
    "id": "7StSP1Jv606J"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# [YOUR CODE HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eb297027-929c-4637-8ed2-64bff31e1fd4",
     "showTitle": false,
     "title": ""
    },
    "id": "OMSgeFA1606J"
   },
   "source": [
    "2. We can obtain the coefficients by using logistic regression model’s attributes. Look at the following plot of the beta coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c1766292-117f-4ea2-89fe-2c62612a600b",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "bh28ROSu606J",
    "outputId": "c208b6b6-2be5-4fbf-ec7b-c6e387a4fd12"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "beta = np.sort(lrModel.coefficients)\n",
    "plt.plot(beta)\n",
    "plt.ylabel('Beta Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a850fb0a-0d4d-4f58-8c23-2aea0db2f3f1",
     "showTitle": false,
     "title": ""
    },
    "id": "K3hE2Ja2606K"
   },
   "source": [
    "3. Use the `.transform()` method to make predictions and save them as `predictions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cdf395d5-b7ef-4f5a-8a32-f1eb76b59122",
     "showTitle": false,
     "title": ""
    },
    "id": "9X_xoxnF606K"
   },
   "outputs": [],
   "source": [
    "predictions = None # [YOUR CODE HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIXVUZLv606K"
   },
   "source": [
    "4. View the first 10 rows of the `predictions` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1AD_mkwl606K",
    "outputId": "0024428e-6085-4f88-80d4-629d3e0c8130"
   },
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-2MVhsn606K"
   },
   "source": [
    "5. What is the area under the curve?\n",
    "\n",
    "    You can find it with the `evaluator.evaluate()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5308cabd-37c2-43d2-bb24-19d638ec54e9",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-bH62bSe606K",
    "outputId": "eb427a7f-ac11-4e3b-dc59-8f96ca0bc602"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "# [YOUR CODE HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9a5ca7ac-9dd1-4610-93c2-41a53c40da74",
     "showTitle": false,
     "title": ""
    },
    "id": "iL4_IuwT606L"
   },
   "source": [
    "## OPTIONAL: HyperParameter Tuning a Gradient-Boosted Tree Classifier\n",
    "\n",
    "1. Fit and make predictions using `GBTClassifier`. The syntax will match what we did above with `LogisticRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e207bd8b-47a0-435a-8e82-affae0da0b9c",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ROwTeFaz606L",
    "outputId": "e4823b97-3e5c-4a18-fb62-4d46e5c31c97"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(maxIter=10)\n",
    "gbtModel = gbt.fit(train)\n",
    "predictions = gbtModel.transform(test)\n",
    "predictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eafee88c-db02-4818-ae96-6165bd61ca1a",
     "showTitle": false,
     "title": ""
    },
    "id": "5Od9M0JV606L"
   },
   "source": [
    "2. Perform cross-validation to compare different parameters.\n",
    "\n",
    "    Note that it can take a while because it's training over many gradient boosted trees. Give it at least 10 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0e9fcb9e-a99a-41fa-bc9f-e58a5f5513bb",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Upc0CJf606L",
    "outputId": "62d9aae2-ed37-491d-b651-1c5ec81d8e2e"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, [2, 4, 6])\n",
    "             .addGrid(gbt.maxBins, [20, 60])\n",
    "             .addGrid(gbt.maxIter, [10, 20])\n",
    "             .build())\n",
    "cv = CrossValidator(estimator=gbt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "cvModel = cv.fit(train)\n",
    "predictions = cvModel.transform(test)\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psf7A_uz606M"
   },
   "source": [
    "## Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HH1MXhRU606M"
   },
   "source": [
    "\n",
    "This notebook is adapted from [Machine Learning with PySpark and MLlib](https://towardsdatascience.com/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark-Live-Assignment-Solution",
   "notebookOrigID": 1487042689144518,
   "widgets": {}
  },
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "sentiment_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "c164bb905a23d5759da3ebff9bf66d00c6b88f2297f9835f615490cebdfaec49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
